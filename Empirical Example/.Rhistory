grid_TADDA_L2_v2_b <- TADDA_L2_v2(grid_y, y_true, epsilon = epsilon)
plot(grid_y, grid_TADDA_L2_b, type = "l", xlab = expression(hat(y)),
ylab = "", ylim = ylim, xlim = xlim)
mtext(expression(TADDA^(2)~with~y > epsilon~", varying"~hat(y)), 3, cex = 0.9, line = 0.3)
rect(-epsilon, -1, epsilon, 11, col = "grey97", border = NA)
abline(h = 0:10, col = "grey95")
abline(v = -6:8, col = "grey95")
box()
lines(grid_y, grid_TADDA_L2_b)
lines(grid_y, grid_TADDA_L2_v1_b - shift, col = "red", lty = "dashed")
lines(grid_y, grid_TADDA_L2_v2_b + shift, col = "blue", lty = "twodash")
abline(v = 0, col = "darkgrey", lty = "dotted")
abline(v = y_true, lty = "solid", col = "darkgrey")
abline(v = c(epsilon, -epsilon), lty = "dotted", col = "darkgrey")
text_in_box(0, 0.9*ylim[2], expression(0), col = "darkgrey")
text_in_box(epsilon, 0.9*ylim[2], expression(epsilon), col = "darkgrey")
text_in_box(-epsilon,  0.9*ylim[2], expression(-epsilon), col = "darkgrey")
text_in_box(y_true,  0.5*ylim[2], expression(y), col = "darkgrey")
# with fixed y_hat, as a function of y_hat
# y_hat in [-epsilon, epsilon]
y_hat <- 0.03
grid_TADDA_L2_c <- TADDA_L2(y_hat, grid_y)
grid_TADDA_L2_v1_c <- TADDA_L2_v1(y_hat, grid_y, epsilon = epsilon)
grid_TADDA_L2_v2_c <- TADDA_L2_v2(y_hat, grid_y, epsilon = epsilon)
plot(grid_y, grid_TADDA_L2_c, type = "l", xlab = expression(y),
ylab = ylab, ylim = ylim, xlim = xlim)
mtext(
expression(TADDA^(2)~with~hat(y)%in%paste("[", -epsilon, ",", epsilon, "]"~", varying"~y)),
3, cex = 0.9, line = 0.3)
rect(-epsilon, -1, epsilon, 11, col = "grey97", border = NA)
abline(h = 0:10, col = "grey95")
abline(v = -6:8, col = "grey95")
box()
lines(grid_y, grid_TADDA_L2_c)
lines(grid_y, grid_TADDA_L2_v1_c - shift, col = "red", lty = "dashed")
lines(grid_y, grid_TADDA_L2_v2_c + shift, col = "blue", lty = "twodash")
abline(v = 0, col = "darkgrey", lty = "dotted")
abline(v = y_hat, lty = "solid", col = "darkgrey")
abline(v = c(epsilon, -epsilon), lty = "dotted", col = "darkgrey")
text_in_box(0, 0.9*ylim[2], expression(0), col = "darkgrey")
text_in_box(epsilon, 0.9*ylim[2], expression(epsilon), col = "darkgrey")
text_in_box(-epsilon,  0.9*ylim[2], expression(-epsilon), col = "darkgrey")
text_in_box(y_hat,  0.5*ylim[2], expression(hat(y)), col = "darkgrey")
# y > epsilon
y_hat <- 0.1
grid_TADDA_L2_d <- TADDA_L2(y_hat, grid_y)
grid_TADDA_L2_v1_d <- TADDA_L2_v1(y_hat, grid_y, epsilon = epsilon)
grid_TADDA_L2_v2_d <- TADDA_L2_v2(y_hat, grid_y, epsilon = epsilon)
plot(grid_y, grid_TADDA_L2_d, type = "l", xlab = expression(y),
ylab = "", ylim = ylim, xlim = xlim)
mtext(expression(TADDA^(2)~with~hat(y) > epsilon~", varying"~y), 3, cex = 0.9, line = 0.3)
rect(-epsilon, -1, epsilon, 11, col = "grey97", border = NA)
abline(h = 0:10, col = "grey95")
abline(v = -6:8, col = "grey95")
box()
lines(grid_y, grid_TADDA_L2_d)
lines(grid_y, grid_TADDA_L2_v1_d - shift, col = "red", lty = "dashed")
lines(grid_y, grid_TADDA_L2_v2_d + shift, col = "blue", lty = "twodash")
abline(v = 0, col = "darkgrey", lty = "dotted")
abline(v = y_true, lty = "solid", col = "darkgrey")
abline(v = c(epsilon, -epsilon), lty = "dotted", col = "darkgrey")
text_in_box(0, 0.9*ylim[2], expression(0), col = "darkgrey")
text_in_box(epsilon, 0.9*ylim[2], expression(epsilon), col = "darkgrey")
text_in_box(-epsilon,  0.9*ylim[2], expression(-epsilon), col = "darkgrey")
text_in_box(y_true,  0.5*ylim[2], expression(hat(y)), col = "darkgrey")
par(mar = c(0, 0, 0, 0))
plot(NULL, xlim = 0:1, ylim = 0:1, xlab = "", ylab = "", axes = FALSE)
legend("center", legend = c(expression({TADDA[0]^(2)}(hat(y), y)),
expression({TADDA1[epsilon]^(2)}(hat(y), y)),
expression({TADDA2[epsilon]^"(2)"}(hat(y), y))),
bty = "n", cex = 1.2, lty = c("solid", "dashed", "twodash"), col = c("black", "red", "blue"),
y.intersp = 1.5)
dev.off()
library(sn)
current_path <- rstudioapi::getActiveDocumentContext()$path # get path of this file
setwd(dirname(current_path))
source("functions.R")
# epsilon:
epsilon <- 0.5 # using larger epsilon for illustration
# parameters of skew normal:
xi <- -1.7
omega <- 5
alpha <- 8
# generate samples:
set.seed(123)
samples_y <- rsn(100000, xi = xi, omega = omega, alpha = alpha)
# prepare plotting density:
grid_y <- seq(from = -5, to = 10, by = 0.01)
F <- psn(x = grid_y, xi = xi, omega = omega, alpha = alpha)
# compute mean and median:
(mu <- mean(samples_y))
(med <- median(samples_y))
# evaluate density at epsilon and minus epsilon
F_minus_epsilon <-  mean(samples_y < -epsilon)
library(sn)
current_path <- rstudioapi::getActiveDocumentContext()$path # get path of this file
setwd(dirname(current_path))
source("functions.R")
# epsilon:
epsilon <- 0.5 # using larger epsilon for illustration
# parameters of skew normal:
xi <- -1.7
omega <- 5
alpha <- 8
# generate samples:
set.seed(123)
samples_y <- rsn(100000, xi = xi, omega = omega, alpha = alpha)
# prepare plotting density:
grid_y <- seq(from = -5, to = 10, by = 0.01)
F <- psn(x = grid_y, xi = xi, omega = omega, alpha = alpha)
# compute mean and median:
(mu <- mean(samples_y))
(med <- median(samples_y))
# evaluate density at epsilon and minus epsilon
F_minus_epsilon <-  mean(samples_y < -epsilon)
F_epsilon <-  mean(samples_y < epsilon)
# inflate with epsilons:
n_epsilon <- sum(samples_y < -epsilon)
samples_y_epsilon <- c(samples_y, rep(epsilon, n_epsilon))
# prepare plotting density of epsilon-inflated distribution:
grid_p <- 1:100/100
quantiles_y_epsilon <- quantile(samples_y_epsilon, p = grid_p)
# obtain median and mean of inflated distribution:
(med_modified <- max((quantile(samples_y, 0.5*(1 - sign(med)*F_minus_epsilon))), 0))
(mu_modified <- mean(samples_y)/(1 + pi))
# Plot:
pdf("Figures/F_vs_G_epsilon.pdf", width = 6, height = 3.5)
par(las = 1, mar = c(4.2, 4.2, 0.5, 0.5))
plot(grid_y, F, type = "l", col = "black", xlab = "y", ylab = "cumulative density", ylim = 0:1)
# highlight tolerance region:
rect(-epsilon, 0, epsilon, 1.2, col = "grey97", border = NA)
# adding vertical / horizontal lines and labelling:
abline(v = -epsilon, col = "lightgrey")
abline(v = epsilon, col = "lightgrey")
library(sn)
current_path <- rstudioapi::getActiveDocumentContext()$path # get path of this file
setwd(dirname(current_path))
source("functions.R")
# epsilon:
epsilon <- 0.5 # using larger epsilon for illustration
# parameters of skew normal:
xi <- -1.7
omega <- 5
alpha <- 8
# generate samples:
set.seed(123)
samples_y <- rsn(100000, xi = xi, omega = omega, alpha = alpha)
# prepare plotting density:
grid_y <- seq(from = -5, to = 10, by = 0.01)
F <- psn(x = grid_y, xi = xi, omega = omega, alpha = alpha)
# compute mean and median:
(mu <- mean(samples_y))
(med <- median(samples_y))
# evaluate density at epsilon and minus epsilon
F_minus_epsilon <-  mean(samples_y < -epsilon)
F_epsilon <-  mean(samples_y < epsilon)
# inflate with epsilons:
n_epsilon <- sum(samples_y < -epsilon)
samples_y_epsilon <- c(samples_y, rep(epsilon, n_epsilon))
# prepare plotting density of epsilon-inflated distribution:
grid_p <- 1:100/100
quantiles_y_epsilon <- quantile(samples_y_epsilon, p = grid_p)
# obtain median and mean of inflated distribution:
(med_modified <- max((quantile(samples_y, 0.5*(1 - sign(med)*F_minus_epsilon))), 0))
(mu_modified <- mean(samples_y)/(1 + pi))
# Plot:
pdf("Figures/F_vs_G_epsilon.pdf", width = 6, height = 3.5)
par(las = 1, mar = c(4.2, 4.2, 0.5, 0.5))
plot(grid_y, F, type = "l", col = "black", xlab = "y", ylab = "cumulative density", ylim = 0:1)
# highlight tolerance region:
rect(-epsilon, 0, epsilon, 1.2, col = "grey97", border = NA)
# adding vertical / horizontal lines and labelling:
abline(v = -epsilon, col = "lightgrey")
abline(v = epsilon, col = "lightgrey")
abline(h = 0.5, col = "lightgrey", lty  ="dashed")
abline(h = F_epsilon/(1 + F_minus_epsilon), col = "lightgrey")
abline(h = (F_epsilon + F_minus_epsilon)/(1 + F_minus_epsilon), col = "lightgrey")
abline(v = med, col = "black", lty = "dashed")
abline(v = med_modified, col = "red", lty  ="dashed")
# adding CDFs:
lines(grid_y, F)
lines(quantiles_y_epsilon, grid_p, col = "red")
# some more labelling:
text_in_box(-3, 0.5, "0.5", col = "lightgrey")
text_in_box(epsilon, 0.7, expression(epsilon), col = "lightgrey")
text_in_box(-epsilon, 0.7, expression(-epsilon), col = "lightgrey")
text_in_box(med_modified, 0.95, "OPF: median of Z", col = "red")
text_in_box(med + 1.25, 0.05, "median of Y", col = "black")
text_in_box(6, (F_epsilon + F_minus_epsilon)/(1 + F_minus_epsilon),
expression((F(epsilon) + pi["-"])/(1 + pi["-"])), col = "lightgrey", cex = 0.8)
text_in_box(6, F_epsilon/(1 + F_minus_epsilon),
expression(F(epsilon)/(1 + pi["-"])), col = "lightgrey", cex = 0.8)
legend("topleft", legend = c("CDF of Y", "CDF of Z"), col = c("black", "red"), lty = 1, bty = "n")
box()
dev.off()
# epsilon:
epsilon <- 0.5 # larger epsilon for illustration:
# parameters of skew normal:
xi <- 1.7
omega <- 5
alpha <- -8
# generate samples:
set.seed(123)
samples_y <- rsn(100000, xi = xi, omega = omega, alpha = alpha)
# prepare plotting density:
grid_y <- seq(from = -10, to = 5, by = 0.01)
F <- psn(x = grid_y, xi = xi, omega = omega, alpha = alpha)
# compute mean and median:
(mu <- mean(samples_y))
(med <- median(samples_y))
# evaluate density at epsilon and minus epsilon
F_minus_epsilon <-  mean(samples_y < -epsilon)
F_epsilon <-  mean(samples_y < epsilon)
# inflate with -epsilons:
n_minus_epsilon <- sum(samples_y > epsilon)
samples_y_minus_epsilon <- c(samples_y, rep(-epsilon, n_minus_epsilon))
# prepare plotting density of -epsilon-inflated distribution:
grid_p <- 1:100/100
quantiles_y_minus_epsilon <- quantile(samples_y_minus_epsilon, p = grid_p)
# obtain median and mean of inflated distribution:
(med_modified <- quantile(samples_y, 0.5*(2 - F_epsilon)))
(mu_modified <- mean(samples_y)/(1 + pi))
pdf("Figures/F_vs_G_minus_epsilon.pdf", width = 6, height = 3.5)
par(las = 1, mar = c(4.2, 4.2, 0.5, 0.5))
plot(grid_y, F, type = "l", col = "black", xlab = "y", ylab = "cumulative density", ylim = 0:1)
# highlight tolerance region:
rect(-epsilon, 0, epsilon, 1.2, col = "grey97", border = NA)
# various vertical and horizontal lines with labels:
abline(v = -epsilon, col = "lightgrey")
abline(v = epsilon, col = "lightgrey")
abline(h = 0.5, col = "lightgrey", lty = "dashed")
abline(h = F_minus_epsilon/(2 - F_epsilon), col = "lightgrey")
abline(h = (1 - F_epsilon + F_minus_epsilon)/(2 - F_epsilon), col = "lightgrey")
abline(v = med, col = "black", lty = "dashed")
abline(v = med_modified, col = "red", lty  ="dashed")
# add CDFs:
lines(quantiles_y_minus_epsilon, grid_p, col = "red")
lines(grid_y, F)
# some more labelling:
text_in_box(3, 0.5, "0.5", col = "lightgrey")
text_in_box(epsilon, 0.3, expression(epsilon), col = "lightgrey")
text_in_box(-epsilon, 0.3, expression(-epsilon), col = "lightgrey")
text_in_box(-6, F_minus_epsilon/(2 - F_epsilon),
expression(F(-epsilon)/(1 + pi["+"])), col = "lightgrey", cex = 0.8)
text_in_box(-5.5, (1 - F_epsilon + F_minus_epsilon)/(2 - F_epsilon),
expression((F(-epsilon) + pi["+"])/(1 + pi["+"])), col = "lightgrey", cex = 0.8)
text_in_box(med_modified + 1.6, 0.15, "OPF: median of Z", col = "red")
text_in_box(med, 0.95, "median of Y", col = "black")
# legend:
legend("topleft", legend = c("CDF of Y", "CDF of Z"), col = c("black", "red"), lty = 1, bty = "n")
box()
dev.off()
# ---------------
### Part 0: Initialization
# ---------------
# load packages
library(tidyverse) # includes dplyr, required for "reduce" function
library(rlist) # functions for manipulation of lists
# set working directory
current_path = rstudioapi::getActiveDocumentContext()$path # get path of this file
setwd(dirname(current_path))
# get scoring functions
source("../Simulations/functions.R")
source("bayes_acts_functions.R")
# read in data
data_fatalities <- read.csv(paste("Data/fatalities.csv"))[,-1]
country_names <- unique(data_fatalities$country_name)
# set parameters & choose task windows
epsilon <- 0.048
# task 2
train_month_id_task2 <- 409:444 # training set (01/14-12/16)
pred_month_id_task2 <- 445:480 # test set (01/17-12/19)
# overall data window
# data generally contains obs. for month_id 121 onwards, 7-step ahead forecast exists for 128 onwards;
# however, data for South Sudan (incl. 7 step-ahead forecast) is only available from 386 onwards so we choose 386 as the starting month
window_months <- 386:495
# TADDA-score that was used in paper: TADDA1 = TADDA1_L1 with epsilon = 0.048
OPF_names <- c("OPF_SE", "OPF_TADDA1")
# initialise objects
predictions <- list()
# mean scores of VIEWS competition ensemble (from Table 2 in Vesco et al., 2022)
MSE_ensemble_cm <- c(.504, .551, .579, .548, .573, .599, mean(c(.504, .551, .579, .548, .573, .599)))
TADDA_ensemble_cm <- c(.371, .379, .394, .381, .386, .400, mean(c(.371, .379, .394, .381, .386, .400)))
# ---------------
### Part 1: Determine optimal window length based on averages losses on training set of task 2
# ---------------
loss_means_df_train <- loss_means_df_pred <- data.frame(matrix(NA, ncol = 6, nrow = 17))
colnames(loss_means_df_train) <- colnames(loss_means_df_pred) <- c("MSE_OPF_SE", "MSE_OPF_TADDA1", "MSE_No_Change",
"MTADDA1_OPF_SE", "MTADDA1_OPF_TADDA1", "MTADDA1_No_Change")
for(window_length in 1:17) { # 17 is highest possible window length, since 409-17-7+1 = 386, the first month for which observations of all countries are available
print(paste("window_length =", window_length))
# compute predictions for each country, score and prediction horizon
for (country in 1:length(country_names)) {
current_country_name <- country_names[country] # print(current_country_name)
data_country <- data_fatalities %>% filter(country_name == current_country_name) %>% filter(month_id %in% window_months)
# construct the log change distribution based on current observation and past fatalities
predictions_ls <- lapply(1:7, function(step_ahead) bayes_acts_predictions(s = step_ahead))
predictions[[country]] <- cbind("country_name" = current_country_name, reduce(predictions_ls, full_join, by = "month_id"))
}
# combine list to data frame and select months that are relevant for evaluation
# predictions are available for (window_months[1] + window_length + s) : window
data_predictions <- merge(data_fatalities, cbind(bind_rows(predictions), "No_Change" = 0), by = c("country_name", "month_id")) %>%
select(., c("country_name", "month_id", starts_with("log_change"), starts_with("OPF_SE"), starts_with("OPF_TADDA1"), "No_Change"))
# compute losses for each prediction horizon and loss function
loss_all <- compute_losses(data_predictions)
# summarize losses
mean_loss_task2_train <- mean_loss(loss_all, train_month_id_task2) # task 2
mean_loss_task2_pred <- mean_loss(loss_all, pred_month_id_task2) # task 2
loss_means_df_train[window_length,] <- mean_loss_task2_train[7,] # stores average loss across window
loss_means_df_pred[window_length,] <- mean_loss_task2_pred[7,] # stores average loss across window
}
# ---------------
### Part 0: Initialization
# ---------------
# load packages
library(tidyverse) # includes dplyr, required for "reduce" function
# ---------------
### Part 0: Initialization
# ---------------
# load packages
library(tidyverse) # includes dplyr, required for "reduce" function
# ---------------
### Part 0: Initialization
# ---------------
# load packages
library(tidyverse) # includes dplyr, required for "reduce" function
library(rlist) # functions for manipulation of lists
# set working directory
current_path = rstudioapi::getActiveDocumentContext()$path # get path of this file
setwd(dirname(current_path))
# get scoring functions
source("../Simulations/functions.R")
source("bayes_acts_functions.R")
# read in data
data_fatalities <- read.csv(paste("Data/fatalities.csv"))[,-1]
country_names <- unique(data_fatalities$country_name)
# set parameters & choose task windows
epsilon <- 0.048
# task 2
train_month_id_task2 <- 409:444 # training set (01/14-12/16)
pred_month_id_task2 <- 445:480 # test set (01/17-12/19)
# overall data window
# data generally contains obs. for month_id 121 onwards, 7-step ahead forecast exists for 128 onwards;
# however, data for South Sudan (incl. 7 step-ahead forecast) is only available from 386 onwards so we choose 386 as the starting month
window_months <- 386:495
# TADDA-score that was used in paper: TADDA1 = TADDA1_L1 with epsilon = 0.048
OPF_names <- c("OPF_SE", "OPF_TADDA1")
# initialise objects
predictions <- list()
# mean scores of VIEWS competition ensemble (from Table 2 in Vesco et al., 2022)
MSE_ensemble_cm <- c(.504, .551, .579, .548, .573, .599, mean(c(.504, .551, .579, .548, .573, .599)))
TADDA_ensemble_cm <- c(.371, .379, .394, .381, .386, .400, mean(c(.371, .379, .394, .381, .386, .400)))
# ---------------
### Part 1: Determine optimal window length based on averages losses on training set of task 2
# ---------------
loss_means_df_train <- loss_means_df_pred <- data.frame(matrix(NA, ncol = 6, nrow = 17))
colnames(loss_means_df_train) <- colnames(loss_means_df_pred) <- c("MSE_OPF_SE", "MSE_OPF_TADDA1", "MSE_No_Change",
"MTADDA1_OPF_SE", "MTADDA1_OPF_TADDA1", "MTADDA1_No_Change")
for(window_length in 1:17) { # 17 is highest possible window length, since 409-17-7+1 = 386, the first month for which observations of all countries are available
print(paste("window_length =", window_length))
# compute predictions for each country, score and prediction horizon
for (country in 1:length(country_names)) {
current_country_name <- country_names[country] # print(current_country_name)
data_country <- data_fatalities %>% filter(country_name == current_country_name) %>% filter(month_id %in% window_months)
# construct the log change distribution based on current observation and past fatalities
predictions_ls <- lapply(1:7, function(step_ahead) bayes_acts_predictions(s = step_ahead))
predictions[[country]] <- cbind("country_name" = current_country_name, reduce(predictions_ls, full_join, by = "month_id"))
}
# combine list to data frame and select months that are relevant for evaluation
# predictions are available for (window_months[1] + window_length + s) : window
data_predictions <- merge(data_fatalities, cbind(bind_rows(predictions), "No_Change" = 0), by = c("country_name", "month_id")) %>%
select(., c("country_name", "month_id", starts_with("log_change"), starts_with("OPF_SE"), starts_with("OPF_TADDA1"), "No_Change"))
# compute losses for each prediction horizon and loss function
loss_all <- compute_losses(data_predictions)
# summarize losses
mean_loss_task2_train <- mean_loss(loss_all, train_month_id_task2) # task 2
mean_loss_task2_pred <- mean_loss(loss_all, pred_month_id_task2) # task 2
loss_means_df_train[window_length,] <- mean_loss_task2_train[7,] # stores average loss across window
loss_means_df_pred[window_length,] <- mean_loss_task2_pred[7,] # stores average loss across window
}
round(loss_means_df_pred[1:17,], 3)
write.csv(round(loss_means_df_train[1:17,], 3), "Results/average_scores_for_different_window_lengths.csv")
# ---------------
### Part 2: Results for chosen window w = 9
# ---------------
window_length <- 9 # use data of the past 9 months for predictions
# compute predictions for each country, score and prediction horizon
for (country in 1:length(country_names)) {
current_country_name <- country_names[country] # print(current_country_name)
data_country <- data_fatalities %>% filter(country_name == current_country_name) %>% filter(month_id %in% window_months)
# construct the log change distribution based on current observation and past fatalities
predictions_ls <- lapply(1:7, function(step_ahead) bayes_acts_predictions(s = step_ahead))
predictions[[country]] <- cbind("country_name" = current_country_name, reduce(predictions_ls, full_join, by = "month_id"))
}
# combine list to data frame and select months that are relevant for evaluation
# predictions are available for (window_months[1] + window_length + s) : window
data_predictions <- merge(data_fatalities, cbind(bind_rows(predictions), "No_Change" = 0), by = c("country_name", "month_id")) %>%
select(., c("country_name", "month_id", starts_with("log_change"), starts_with("OPF_SE"), starts_with("OPF_TADDA1"), "No_Change"))
write.csv(data_predictions, paste("Results/individual_predictions_w", window_length, ".csv", sep = ""))
## 2.1 Summary statistics
data_OPF_TADDA <- unlist(data_predictions %>% filter(month_id %in% pred_month_id_task2) %>%
select(., c(starts_with("OPF_TADDA1"))))
data_OPF_SE <- unlist(data_predictions %>% filter(month_id %in% pred_month_id_task2) %>%
select(., c(starts_with("OPF_SE"))))
data_log_change <- unlist(data_predictions %>% filter(month_id %in% pred_month_id_task2) %>%
select(., c(starts_with("log_change"))))
empirical_quantiles <- rbind(round(quantile(data_OPF_SE, (c(1:4, 15:20))/20), 3),
round(quantile(data_OPF_TADDA, (c(1:4, 15:20))/20), 3),
round(quantile(data_log_change, (c(1:4, 15:20))/20), 3))
rownames(empirical_quantiles) <- c(OPF_names, "True_log_changes")
write.csv2(empirical_quantiles, paste("Results/empirical_quantiles_w", window_length, ".csv", sep = ""))
# the following characteristics are computed across all forecasting horizons s=2,...,7
mean(data_OPF_SE) # average mean forecast
sum(data_OPF_SE==0)/length(data_OPF_SE) # percentage of mean forecasts that are zero
sum(abs(data_OPF_SE)>epsilon)/length(data_OPF_SE) # percentage of mean forecasts outside the epsilon interval
mean(data_OPF_TADDA) # average OPF_TADDA forecast
sum(data_OPF_TADDA==0)/length(data_OPF_TADDA) # percentage of OPF_TADDA forecasts that are zero
sum(abs(data_OPF_TADDA)>epsilon)/length(data_OPF_TADDA) # percentage of OPF_TADDA forecasts outside the epsilon interval
mean(data_log_change) # average true log change
sum(data_log_change==0)/length(data_log_change) # percentage of true log changes that are zero
sum(abs(data_log_change)>epsilon)/length(data_log_change) # percentage of true log_changes outside the epsilon interval
## 2.2
# compute losses for each prediction horizon and loss function
loss_all <- compute_losses(data_predictions)
write.csv(loss_all, paste("Results/individual_losses_w", window_length, ".csv", sep = ""))
# summarise losses
mean_loss_task2_pred <- mean_loss(loss_all, pred_month_id_task2); mean_loss_task2_pred # task 2
mean_loss_task2_pred_incl_ensembles <- data.frame(mean_loss_task2_pred[,1:2], MSE_ensemble_cm, mean_loss_task2_pred[,3:5], TADDA_ensemble_cm, "MTADDA1_No_Change" = mean_loss_task2_pred[,6])
write.csv(round(mean_loss_task2_pred_incl_ensembles, 3), paste("Results/average_scores_w", window_length, ".csv", sep = ""))
# load packages
# library(dplyr) # included in tidyverse
library(tidyverse) # required for "reduce" function
library(rlist) # for data manipulation
library(purrr) # for data manipulation
# set working directory (can also be done manually)
current_path = rstudioapi::getActiveDocumentContext()$path # get path of this file
setwd(dirname(current_path))
# get scoring functions
source("bayes_acts_functions.R")
# read in data
data_fatalities <- read.csv(paste("Data/fatalities.csv"))[,-1]
country_names <- unique(data_fatalities$country_name)
# define week when forecast is issued:
end <- 470
# data subset to plot:
mali <- subset(data_fatalities, country_name == "Mali" & month_id %in% c(445:end))
# time on calendar scale:
mali$time <- mali$year + mali$month/12
# extract last value (needed in computations:)
last_value <- tail(mali$fatalities, 1)
yl <- c(5, 115)
# Plot:
pdf("Figures/example_mali.pdf", width = 9, height = 3.5)
# Plot:
pdf("Figures/example_mali.pdf", width = 9, height = 3.5)
# structure plot area:
layout(matrix(1:2, ncol = 2), widths = c(0.6, 0.4))
par(las = 1, mar = c(2.5, 4, 2.5, 5))
# time series plot:
plot(mali$time, mali$fatalities, type = "h", xlim = c(2017, 2019.5), axes = FALSE,
xlab = "", ylab = "fatalities", ylim = yl,
main = "", log = "y", col = "white")
mtext("Time series of fatalities", line = 1)
# add axes manually:
# left:
axis(1, at = c(2017 + c(1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31)/12),
labels = c("Jan 17", "", "Jul 17", "", "Jan 18", "", "Jul 18", "", "Jan 19", "", "Jul 19"))
axis(2, at = c(5, 10, 20, 50, 100))
# right:
ylabs_right <- -4:2
at_right <- last_value*exp(ylabs_right)
axis(4, at = at_right, labels = ylabs_right)
mtext("log-difference to observation Feb 2019", side = 4, las = 0, line = 2.5)
box()
# highlightinh and labelling:
now <- tail(mali$time, 1)
abline(v = now + 0.2/12, lty = 3)
rect(now - 8.2/12, 0.01, now + 0.2/12, 150, col = rgb(0.2, 0.2, 0.2, 0.2), border = NA)
text(2018.8, 90, "last 9\n observations", cex = 0.75)
text(now + 1.3/12, 25, "last available obsrevation: Feb 2019", srt = 90, cex = 0.75)
abline(h = last_value, col = "darkgrey", lty = "solid")
# add points for last obsrevations:
points(tail(mali$time, 9), tail(mali$fatalities, 9), pch = 16, cex = 0.75)
points(tail(mali$time, 1), tail(mali$fatalities, 1), pch = 4, cex = 1.5)
lines(mali$time, mali$fatalities, type = "h")
# Boxplot:
par(mar = c(2.5, 4, 2.5, 1))
reference_obs <- tail(mali$fatalities, 9)
last_obs <- tail(mali$fatalities, 1)
reference_log_changes <- log(reference_obs + 1) - log(last_obs + 1)
boxplot(reference_log_changes, col = "white", border = "grey", ylim = log(yl/last_value),
ylab = "log change", main = "", axes = FALSE)
axis(2, at = c(-2, -1, 0, 1))
box()
mtext(side = 3, "Forecast distribution\n for log-change")
# compute OPFs:
mu <- mean(reference_log_changes)
TADDA_BA <- OPF_TADDA1_L1(reference_log_changes, epsilon = 0.048)
# add to plot:
abline(h = TADDA_BA, col = "red", lty = 4)
abline(h = mu, col = "black", lty = 2)
abline(h = 0, col = "darkgrey", lty = "solid")
# boxplot on top:
boxplot(reference_log_changes, col = "white", border = "grey", add = TRUE, axes = FALSE)
# add points for individual values:
points(rep(1, length(reference_log_changes)), reference_log_changes, pch = 16, cex = 0.75)
points(1, 0, pch = 4, cex = 1.2)
# legend:
legend("bottom", legend = c(paste("Mean:", round(mu, 2)),
expression(OPF~TADDA[0.048]: -0.048)),
cex = 0.75, bty = "n", lty = c(2, 4), col = c("black", "red"))
dev.off()
